{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOF0mMzZKmJS",
        "outputId": "0211967a-a061-49b3-e7ac-efc0985ddd47"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqY5NNGmefsg"
      },
      "source": [
        "# Tempo evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x55JqhjvWi2"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YzRBeHu4VgX"
      },
      "source": [
        "### Import requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiaeuJxh4R_C"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import librosa\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from functools import reduce\n",
        "import os\n",
        "from numpy import float32\n",
        "from IPython.display import Audio\n",
        "import torchaudio\n",
        "import IPython\n",
        "device = \"cuda\"\n",
        "sr=48000\n",
        "slice_audio_sec = 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P12O8CsBQGp"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0gm5PvWBTcK"
      },
      "outputs": [],
      "source": [
        "def slice_by_second(audio, segment_length_sec, sr=sr):\n",
        "    # Convert segment length to samples\n",
        "    segment_length_samples = int(segment_length_sec * sr)\n",
        "    num_segments = len(audio) // segment_length_samples\n",
        "    segments = []\n",
        "    for i in range(num_segments):\n",
        "        start_sample = i * segment_length_samples\n",
        "        end_sample = (i + 1) * segment_length_samples\n",
        "        segment = audio[start_sample:end_sample]\n",
        "        segments.append(segment)\n",
        "    return segments\n",
        "\n",
        "def parse_for_local(path:str):\n",
        "    is_drive = os.path.isdir('/content/drive/MyDrive/')\n",
        "    if is_drive:\n",
        "        return path\n",
        "    else:\n",
        "        return path.replace(\"/content/drive/MyDrive/\", \"/Users/yoonsookim/Library/CloudStorage/GoogleDrive-ml.laptise@gmail.com/マイドライブ/\")\n",
        "\n",
        "def extract_melspecto(audio_segment):\n",
        "    hop_length = int(1024/ 4)\n",
        "    melspectrogram = librosa.feature.melspectrogram(\n",
        "        y=audio_segment,\n",
        "        sr=sr,\n",
        "        n_fft=1024,\n",
        "        hop_length=hop_length)\n",
        "    return melspectrogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QumW6fpckRT2"
      },
      "source": [
        "### Load audios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Up7s4yHIgQJF",
        "outputId": "9dfcb96d-4d38-4435-d917-7bcb1ad37025"
      },
      "outputs": [],
      "source": [
        "sheet = pd.read_csv(\"https://docs.google.com/spreadsheets/d/1UDQxW1s2D6kUJuYOqQOC5WPmU6UlGb-GTCwoe52_3qw/export?format=csv\")\n",
        "rows = sheet.dropna(subset=[\"BPM\", \"AR\"])\n",
        "# List to store loaded audio data\n",
        "loaded_data = []\n",
        "\n",
        "dset = []\n",
        "# Load audio files\n",
        "for _,row in rows.iterrows():\n",
        "    path = row[\"dir\"]\n",
        "    bpm = row[\"BPM\"]\n",
        "    ar_path = parse_for_local(f\"{path}/ar.wav\")\n",
        "    audio, _ = librosa.load(ar_path, sr=sr, mono=True)\n",
        "    metadata = {\n",
        "        \"bpm\": bpm,\n",
        "        \"ar_path\": ar_path,\n",
        "        \"title\": row[\"Title\"],\n",
        "        'artist': row['Artist'],\n",
        "    }\n",
        "    segments = slice_by_second(audio, slice_audio_sec)\n",
        "    for segment in segments:\n",
        "        features = extract_melspecto(segment)\n",
        "        feature_torch = torch.tensor(features).to(device)\n",
        "        dset.append({\n",
        "            \"features\": feature_torch,\n",
        "            \"bpm\": bpm,\n",
        "            \"metadata\": metadata,\n",
        "            \"segment\": segment\n",
        "        })\n",
        "print(len(dset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxDXkOrL5o50"
      },
      "source": [
        "### Extract Feature and build dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JziroL_W5qqE"
      },
      "outputs": [],
      "source": [
        "def shape_to_value(shape):\n",
        "    return reduce(lambda x, y: x * y, shape)\n",
        "\n",
        "class TempoDataset(Dataset):\n",
        "\n",
        "    def get_input_size(self):\n",
        "        shapes = list(self._dset[0][\"feature\"].shape)\n",
        "        size = reduce(lambda x, y: x * y, shapes)\n",
        "        return size\n",
        "\n",
        "    def _parse_dict(self, d: dict):\n",
        "        return {\n",
        "            \"feature\": d[\"features\"],\n",
        "            \"bpm\": float32(d[\"bpm\"]),\n",
        "            \"metadata\": d[\"metadata\"],\n",
        "            \"segment\": d[\"segment\"]\n",
        "        }\n",
        "\n",
        "    def _check_feature_length(self):\n",
        "        fisrt_shape = self._dset[0][\"feature\"].shape\n",
        "        shapes = []\n",
        "        for d in self._dset:\n",
        "            shape = d[\"feature\"].shape\n",
        "            shapes.append(shape)\n",
        "            if shape != fisrt_shape:\n",
        "                print(shapes)\n",
        "                print(d['metadata'])\n",
        "                raise ValueError(\"All features must have the same shape.\")\n",
        "\n",
        "    def __init__(self, dset: list[dict]):\n",
        "        self._dset = list(map(self._parse_dict, dset))\n",
        "        self._check_feature_length()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._dset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        target = self._dset[idx]\n",
        "        return target[\"feature\"], target[\"bpm\"], target[\"metadata\"], target[\"segment\"]\n",
        "\n",
        "dataset = TempoDataset(dset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaSiCGIW8z3a"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "AKewldzY81Ko",
        "outputId": "f6e71d7a-5c2c-48a9-a37d-0c450f96720a"
      },
      "outputs": [],
      "source": [
        "# Simple model definition\n",
        "class TrainReport:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.histories = []\n",
        "        self.troubles = []\n",
        "\n",
        "    def add_history(self, epoch:int, loss:int, metadata: dict, segment):\n",
        "        self.histories.append({\"epoch\": epoch, \"loss\":loss, \"metadata\": metadata, \"segment\": segment})\n",
        "        is_trouble = self.get_avg_loss() < loss\n",
        "        print(is_trouble)\n",
        "        if is_trouble:\n",
        "            self.regist_troubles()\n",
        "\n",
        "    def get_loss_list(self):\n",
        "        return list(map(lambda x: x[\"loss\"], self.histories))\n",
        "\n",
        "    def get_epoch_list(self):\n",
        "        return list(map(lambda x: x[\"epoch\"], self.histories))\n",
        "\n",
        "    def get_avg_loss(self):\n",
        "        return sum(self.get_loss_list()) / len(self.get_loss_list())\n",
        "\n",
        "    def regist_troubles(self):\n",
        "        last_history = self.histories[-1]\n",
        "        self.troubles.append({\n",
        "            \"epoch\": last_history[\"epoch\"],\n",
        "            \"loss\": last_history[\"loss\"],\n",
        "            \"metadata\": last_history[\"metadata\"],\n",
        "            \"segment\": last_history[\"segment\"],\n",
        "            \"avg_loss\": self.get_avg_loss()\n",
        "        })\n",
        "        print(\"New trouble added\")\n",
        "\n",
        "    def get_troubles(self):\n",
        "        return self.troubles\n",
        "\n",
        "\n",
        "report = TrainReport()\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, input_size:int, hidden_size:int):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, 1)  # 出力層の数は10\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flattern(x)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)  # 出力層（活性化関数なし）\n",
        "        return x\n",
        "\n",
        "    def flattern(self, x):\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "    def predict(self, audio_path:str):\n",
        "        y, *_ = librosa.load(audio_path, sr=sr, mono=True)\n",
        "        sliced, *_ = slice_by_second(y, 30)\n",
        "        melspecto = extract_melspecto(sliced)\n",
        "        torch_melspecto = torch.tensor([melspecto]).to(device)\n",
        "        result = self(torch_melspecto)\n",
        "        return result.item()\n",
        "\n",
        "    def _train(self, dataset: TempoDataset, num_epochs:int,optimizer, criterion=nn.MSELoss()):\n",
        "        # for plot\n",
        "        losses_for_plot = []  # 各エポックのロスを格納するリスト\n",
        "        epoches_for_plot = []\n",
        "        # for log\n",
        "        messages = \"\"\n",
        "\n",
        "        dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "        step = 0\n",
        "        # Training loop\n",
        "        for epoch in range(num_epochs):\n",
        "            # Each Step\n",
        "            for feature, bpm, metadata, segment in dataloader:\n",
        "                # Zero the gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self(feature.to(device))\n",
        "                # Calculate loss\n",
        "                loss = criterion(outputs.to(device), bpm.to(device))\n",
        "                # Backward pass and optimization\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                step += 1\n",
        "\n",
        "            clear_output()\n",
        "            # Each Epoch\n",
        "            new_epoch = epoch + 1\n",
        "            report.add_history(epoch=new_epoch, loss=loss.item(), metadata=metadata, segment=segment)\n",
        "            messages += f'Epoch [{new_epoch}/{num_epochs}], Loss: {loss.item()}, Answer: {bpm.item()}, Predicted: {outputs.item()}\\n'\n",
        "            #plot\n",
        "            plt.plot(report.get_epoch_list(), report.get_loss_list())\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.show()\n",
        "            #log\n",
        "            print(messages)\n",
        "            if new_epoch % 5 == 0:\n",
        "                torch.save(self.state_dict(), f\"/content/drive/MyDrive/audio/models/tempo-predictor/e_{new_epoch}.pth\")\n",
        "\n",
        "input_size = dataset.get_input_size()\n",
        "print(f\"Input size is {input_size}\")\n",
        "hidden_size = 1600\n",
        "\n",
        "model = SimpleModel(input_size, hidden_size).to(device)\n",
        "\n",
        "lowest_loss_model = {\n",
        "    \"model\": None,\n",
        "    \"loss\": 1000\n",
        "}\n",
        "\n",
        "model._train(\n",
        "    dataset=dataset,\n",
        "    num_epochs=1000,\n",
        "    criterion=nn.MSELoss(),\n",
        "    optimizer=optim.Adam(model.parameters(), lr=1e-7)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bz__tCuMdFjw",
        "outputId": "859edc91-207d-47b6-ea00-4aee95e41d0f"
      },
      "outputs": [],
      "source": [
        "\n",
        "troubles = report.get_troubles()\n",
        "for trouble in troubles:\n",
        "    segment = trouble[\"segment\"][0]\n",
        "    loss = trouble[\"loss\"]\n",
        "    epoch = trouble[\"epoch\"]\n",
        "    metadata = trouble[\"metadata\"]\n",
        "    # print(f\"{segment}\")\n",
        "    x = segment.to('cpu').detach().numpy().copy()\n",
        "    # print(list(segment))\n",
        "\n",
        "    # torchaudio.load(x, sr)\n",
        "    # print(segment)\n",
        "    print(f\"{loss}\")\n",
        "    print(epoch)\n",
        "\n",
        "    IPython.display.display(IPython.display.Audio(x, rate=sr))\n",
        "    # Audio(x, rate=sr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmnYd5oLAFZL"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gIMwM2GAHPa",
        "outputId": "2b16f524-029f-4128-fb64-7cfdec919613"
      },
      "outputs": [],
      "source": [
        "file_path=\"/content/drive/MyDrive/audio/batch/MrChu-unknown/ar.wav\"\n",
        "\n",
        "import numpy as np\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "duration = 30\n",
        "x_sr = 200\n",
        "bpm_min, bpm_max = 60, 240\n",
        "\n",
        "# 楽曲の信号を読み込む\n",
        "y, sr = librosa.load(file_path, offset=38, duration=duration, mono=True)\n",
        "\n",
        "# ビート検出用信号の生成\n",
        "# リサンプリング & パワー信号の抽出\n",
        "x = np.abs(librosa.resample(y=y, orig_sr=sr, target_sr=x_sr)) ** 2\n",
        "x_len = len(x)\n",
        "\n",
        "# 各BPMに対応する複素正弦波行列を生成\n",
        "M = np.zeros((bpm_max, x_len), dtype=complex)\n",
        "for bpm in range(bpm_min, bpm_max):\n",
        "    thete = 2 * np.pi * (bpm/60) * (np.arange(0, x_len) / x_sr)\n",
        "    M[bpm] = np.exp(-1j * thete)\n",
        "\n",
        "# 各BPMとのマッチング度合い計算\n",
        "#（複素正弦波行列とビート検出用信号との内積）\n",
        "x_bpm = np.abs(np.dot(M, x))\n",
        "\n",
        "# BPM　を算出\n",
        "bpm = np.argmax(x_bpm)\n",
        "\n",
        "onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
        "tempo = librosa.feature.rhythm.tempo(onset_envelope=onset_env, sr=sr)\n",
        "\n",
        "\n",
        "### Deep learning way\n",
        "model = SimpleModel(input_size, hidden_size).to(device)\n",
        "model.eval()\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/audio/models/tempo-predictor/e_100.pth'))\n",
        "\n",
        "# Perform prediction\n",
        "with torch.no_grad():\n",
        "    predicted = model.predict(parse_for_local(file_path))\n",
        "    print(f\"Deeplearning BPM: {predicted}\")\n",
        "    print(f\"librosa BPM: {tempo}\")\n",
        "    print(f\"legacy BPM: {bpm}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cARrn_nHux9W"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "8P12O8CsBQGp",
        "QumW6fpckRT2",
        "gxDXkOrL5o50"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
