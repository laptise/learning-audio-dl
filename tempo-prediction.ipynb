{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqY5NNGmefsg"
      },
      "source": [
        "# Tempo evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YzRBeHu4VgX"
      },
      "source": [
        "### Import requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "PiaeuJxh4R_C"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import librosa\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from AudioFeatureExtractor import AudioFeatureExtractor\n",
        "from functools import reduce\n",
        "import os\n",
        "device = \"mps\"\n",
        "sr=22050"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P12O8CsBQGp"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "id": "F0gm5PvWBTcK"
      },
      "outputs": [],
      "source": [
        "def slice_by_second(audio, segment_length_sec, sr=sr):\n",
        "    # Convert segment length to samples\n",
        "    segment_length_samples = int(segment_length_sec * sr)\n",
        "    num_segments = len(audio) // segment_length_samples\n",
        "    segments = []\n",
        "    for i in range(num_segments):\n",
        "        start_sample = i * segment_length_samples\n",
        "        end_sample = (i + 1) * segment_length_samples\n",
        "        segment = audio[start_sample:end_sample]\n",
        "        segments.append(segment)\n",
        "    return segments\n",
        "\n",
        "def parse_for_local(path:str):\n",
        "    is_drive = os.path.isdir('/content/drive/MyDrive/')\n",
        "    if is_drive:\n",
        "        return path\n",
        "    else:\n",
        "        return path.replace(\"/content/drive/MyDrive/\", \"/Users/yoonsookim/Library/CloudStorage/GoogleDrive-ml.laptise@gmail.com/マイドライブ/\")\n",
        "\n",
        "# Function to extract MFCC features from audio segment\n",
        "def extract_melspecto(audio_segment):\n",
        "    hop_length = int(1024/ 4)\n",
        "    melspectrogram = librosa.feature.melspectrogram(\n",
        "        y=audio_segment,\n",
        "        sr=sr,\n",
        "        n_fft=1024,\n",
        "        hop_length=hop_length)\n",
        "    return melspectrogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QumW6fpckRT2"
      },
      "source": [
        "### Load audios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Up7s4yHIgQJF",
        "outputId": "23cfe956-fbe2-425f-ecb8-92c3a95f44f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "372\n"
          ]
        }
      ],
      "source": [
        "sheet = pd.read_csv(\"https://docs.google.com/spreadsheets/d/1UDQxW1s2D6kUJuYOqQOC5WPmU6UlGb-GTCwoe52_3qw/export?format=csv\")\n",
        "rows = sheet.dropna(subset=[\"BPM\", \"AR\"])\n",
        "# List to store loaded audio data\n",
        "loaded_data = []\n",
        "\n",
        "dset = []\n",
        "# Load audio files\n",
        "for _,row in rows.iterrows():\n",
        "    path = row[\"dir\"]\n",
        "    bpm = row[\"BPM\"]\n",
        "    ar_path = parse_for_local(f\"{path}/ar.wav\")\n",
        "    audio, _ = librosa.load(ar_path, sr=sr, mono=True)\n",
        "    metadata = {\n",
        "        \"bpm\": bpm,\n",
        "        \"ar_path\": ar_path,\n",
        "        \"title\": row[\"Title\"],\n",
        "        'artist': row['Artist'],\n",
        "    }\n",
        "    segments = slice_by_second(audio, 10)\n",
        "    for segment in segments:\n",
        "        features = extract_melspecto(segment)\n",
        "        feature_torch = torch.tensor(features)\n",
        "        dset.append({\n",
        "            \"features\": feature_torch,\n",
        "            \"bpm\": bpm,\n",
        "            \"metadata\": metadata\n",
        "        })\n",
        "print(len(dset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxDXkOrL5o50"
      },
      "source": [
        "### Extract Feature and build dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JziroL_W5qqE",
        "outputId": "2741f0d0-6e95-4f55-89e3-29a1ed91fb21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862]), torch.Size([128, 862])]\n"
          ]
        }
      ],
      "source": [
        "from numpy import float32\n",
        "\n",
        "\n",
        "def pick_values(dset, key):\n",
        "    return list(map(lambda d: d[key], dset))\n",
        "\n",
        "class TempoDataset(Dataset):\n",
        "    \n",
        "    def get_input_size(self):\n",
        "        shapes = list(self._dset[0][\"feature\"].shape)\n",
        "        size = reduce(lambda x, y: x * y, shapes)\n",
        "        return size\n",
        "    \n",
        "    def _parse_dict(self, d: dict):\n",
        "        return {\n",
        "            \"feature\": d[\"features\"],\n",
        "            \"bpm\": float32(d[\"bpm\"]),\n",
        "            \"metadata\": d[\"metadata\"]\n",
        "        }\n",
        "\n",
        "    def _check_feature_length(self):\n",
        "        fisrt_shape = self._dset[0][\"feature\"].shape\n",
        "        shapes = []\n",
        "        for d in self._dset:\n",
        "            shape = d[\"feature\"].shape\n",
        "            shapes.append(shape)\n",
        "            if shape != fisrt_shape:\n",
        "                print(shapes)\n",
        "                print(d['metadata'])\n",
        "                raise ValueError(\"All features must have the same shape.\")\n",
        "\n",
        "    def __init__(self, dset: list[dict]):\n",
        "        self._dset = list(map(self._parse_dict, dset))\n",
        "        self._check_feature_length()\n",
        "    def __len__(self):\n",
        "        return len(self._dset)\n",
        "    def __getitem__(self, idx):\n",
        "        target = self._dset[idx]\n",
        "        return target[\"feature\"], target[\"bpm\"], target[\"metadata\"]\n",
        "\n",
        "dataset = TempoDataset(dset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaSiCGIW8z3a"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AKewldzY81Ko",
        "outputId": "a01f6878-4e9a-4fa1-f1dc-5b312b167bf7"
      },
      "outputs": [],
      "source": [
        "# Simple model definition\n",
        "import random\n",
        "\n",
        "\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, input_size:int, hidden_size:int):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, 1)  # 出力層の数は10\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flattern(x)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)  # 出力層（活性化関数なし）\n",
        "        return x\n",
        "    \n",
        "    def flattern(self, x):\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "    def predict(self, audio_path:str):\n",
        "        y, *_ = librosa.load(audio_path, sr=sr, mono=True)\n",
        "        sliced, *_ = slice_by_second(y, 5)\n",
        "        melspecto = extract_melspecto(sliced)\n",
        "        torch_melspecto = torch.tensor([melspecto]).to(device)\n",
        "        result = self(torch_melspecto)\n",
        "        return result.item()\n",
        "    \n",
        "    def train(self, dataset: TempoDataset, num_epochs:int,optimizer, criterion=nn.MSELoss()):\n",
        "        # for plot\n",
        "        losses = []  # 各エポックのロスを格納するリスト\n",
        "        epoches = []\n",
        "        # for log\n",
        "        messages = \"\"\n",
        "\n",
        "        dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "        step = 0\n",
        "        # Training loop\n",
        "        for epoch in range(num_epochs):\n",
        "            # Each Step\n",
        "            for feature, bpm, metadata in dataloader:\n",
        "                # Zero the gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self(feature.to(device))\n",
        "                # Calculate loss\n",
        "                loss = criterion(outputs.to(device), bpm.to(device))\n",
        "                # Backward pass and optimization\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                step += 1\n",
        "            \n",
        "            # Each Epoch\n",
        "            new_epoch = epoch + 1\n",
        "            epoches.append(new_epoch)\n",
        "            losses.append(loss.item())\n",
        "            if loss < lowest_loss_model[\"loss\"]:\n",
        "                lowest_loss_model[\"model\"] = model\n",
        "                lowest_loss_model[\"loss\"] = loss\n",
        "                print(f\"Lowest loss so far: {lowest_loss_model['loss']}\")\n",
        "            clear_output()\n",
        "            messages += f'Epoch [{new_epoch}/{num_epochs}], Loss: {loss.item()}, Answer: {bpm.item()}, Predicted: {outputs.item()}\\n'\n",
        "            #plot\n",
        "            plt.plot(epoches, losses)\n",
        "            plt.show()\n",
        "            #log\n",
        "            print(messages)\n",
        "            \n",
        "input_size = dataset.get_input_size()\n",
        "hidden_size = 200 \n",
        "\n",
        "model = SimpleModel(input_size, hidden_size).to(device)\n",
        "\n",
        "lowest_loss_model = {\n",
        "    \"model\": None,\n",
        "    \"loss\": 1000\n",
        "}\n",
        "\n",
        "model.train(\n",
        "    dataset=dataset,\n",
        "    num_epochs=1000,\n",
        "    criterion=nn.MSELoss(),\n",
        "    optimizer=optim.Adam(model.parameters(), lr=0.0001)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmnYd5oLAFZL"
      },
      "source": [
        "### Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gIMwM2GAHPa",
        "outputId": "e7e2ede5-a891-46d4-9932-e748a6735fbb"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "file_path=\"/content/drive/MyDrive/audio/batch/Anarchy-hige/ar.wav\"\n",
        "\n",
        "# Perform prediction\n",
        "with torch.no_grad():\n",
        "    predicted = model.predict(parse_for_local(file_path))\n",
        "    print(predicted)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "o3UYYvaRAXf0"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
